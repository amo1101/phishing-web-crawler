# Global paths
state_db: "/home/uowadmin/iosco/data/state.db"

iosco:
  csv_root: "/home/uowadmin/iosco/data/csv"
  nca_id: ""        # e.g., "64" for NZ; leave empty for all NCAs
  subsection: "main"
  request_timeout_seconds: 60

# Scheduling
schedule:
  base_date: "2025-09-01"
  daily_run_time: "02:00"     # local time for the daily ingestion + orchestration (24h)
  job_interval_days: 7

queue:
  max_parallel_crawl_jobs: 5
  max_parallel_download_jobs: 1
  max_retries: 3 # now applies only to wayback download jobs
  reconcile_interval_seconds: 30 # how often the worker polls Browsertrix or wb downloader to refresh job statuses

# Liveness
liveness:
  timeout_seconds: 10
  treat_http_4xx_as_live: true
  max_parallel_probes: 30

# Browsertrix integration
browsertrix:
  base_url: "http://localhost:30870"
  username: "admin@example.com"
  password: "PASSWORD!"
  crawler_setting:
    max_time: 3600
    max_pages: 20000
    max_size: 3221225472

# Dead-site acquisition
wb_downloader:
  output_dir: "/home/uowadmin/iosco/data/wayback"
  concurrency: 3          # how many files to download in parallel for Wayback downloads

# Dashboard
web:
  enable_status_page: true
  host: "127.0.0.1"
  port: 8090
  basic_auth:
    enabled: false
    username: "admin"
    password: "admin"

logging:
  level: DEBUG                 # DEBUG | INFO | WARNING | ERROR
  file: "/home/uowadmin/iosco/crawler.log"
  rotate:
    when: "midnight"          # Timed rotation; alternatives: "S","M","H","D","W0"-"W6"
    backupCount: 14           # keep last 14 files
  console: true               # also log to stdout (useful in dev)
  json: false                 # set true if you prefer JSON logs
