# Global paths
state_db: /Data/scratch/cw486/phishingwebarchive/run/state.db

iosco:
  csv_root: "/Data/scratch/cw486/phishingwebarchive/run/iosco"
  nca_id: 64
  subsection: "main"
  request_timeout_seconds: 60

# Scheduling
schedule:
  base_date: "2025-10-28"
  daily_run_time: "02:00"     # local time for the daily ingestion + orchestration (24h)
  job_interval_days: 7

queue:
  max_parallel_jobs: 5           # how many queue jobs can be RUNNING at once
  reconcile_interval_seconds: 15 # how often the worker polls Browsertrix or wb downloader to refresh job statuses

# Liveness
liveness:
  timeout_seconds: 10
  treat_http_4xx_as_live: true
  max_parallel_probes: 30

# Browsertrix integration
browsertrix:
  base_url: "http://localhost:30870"
  username: "admin"
  password: "admin"
  crawler_setting:
    max_time: 3600
    max_pages: 20000
    max_size: 3221225472

# Dead-site acquisition
wb_downloader:
  downloader: "/Data/scratch/cw486/phishingwebarchive/run/wb"
  output_dir: "/Data/scratch/cw486/phishingwebarchive/run/wb"
  concurrency: 3          # how many files to download in parallel for Wayback downloads

# Dashboard
web:
  enable_status_page: true
  host: "127.0.0.1"
  port: 8090
  basic_auth:
    enabled: false
    username: "admin"
    password: "admin"

logging:
  level: DEBUG                 # DEBUG | INFO | WARNING | ERROR
  file: "/Data/scratch/cw486/phishingwebarchive/run/crawler.log"
  rotate:
    when: "midnight"          # Timed rotation; alternatives: "S","M","H","D","W0"-"W6"
    backupCount: 14           # keep last 14 files
  console: true               # also log to stdout (useful in dev)
  json: false                 # set true if you prefer JSON logs
